{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5aec7700",
   "metadata": {},
   "source": [
    "# Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498bd91e",
   "metadata": {},
   "source": [
    "### Загрузка "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "564919db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader_1 = PyPDFLoader(\n",
    "    \"../data/standard_data/sql_primer.pdf\",\n",
    ")\n",
    "\n",
    "loader_2 = PyPDFLoader(\n",
    "    \"../data/standard_data/Tutorial_PostgreSQL.pdf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f622cd",
   "metadata": {},
   "source": [
    "### Соединение данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d83adea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader_1.load()\n",
    "docs += loader_2.load()\n",
    "\n",
    "print(len(docs)) # разбиене на веторы требует больших мощностей, в связи с чем обез данных\n",
    "docs = docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c008a1d",
   "metadata": {},
   "source": [
    "### Обработка данных от html и прочих символов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a120ee9f",
   "metadata": {},
   "source": [
    "Обработка не доделана с ней надо подробне разбраться, смотреть как работает.\n",
    "* у нас есть код, смотрите чтоб его не снесли"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5786f70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "nlp = spacy.load(\"ru_core_news_sm\")  # Замените на нужную модель\n",
    "\n",
    "# Функция для очистки текста\n",
    "def clean_text(doc):\n",
    "    # Удаление HTML-тегов с помощью BeautifulSoup\n",
    "    soup = BeautifulSoup(doc.page_content, \"html.parser\")\n",
    "    clean_text = soup.get_text()\n",
    "\n",
    "    # Обработка текста с помощью spaCy\n",
    "    doc_spacy = nlp(clean_text)\n",
    "\n",
    "    # Удаление лишних символов и создание очищенного текста\n",
    "    cleaned_tokens = [token.text for token in doc_spacy if not token.is_punct and not token.is_space]\n",
    "    cleaned_text = \" \".join(cleaned_tokens)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "# Применение функции очистки к документам\n",
    "cleaned_content = []\n",
    "for document in docs:\n",
    "    cleaned_content.append(clean_text(document))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53911c2",
   "metadata": {},
   "source": [
    "### Перевод в формат для векторизации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44237f2e",
   "metadata": {},
   "source": [
    "Перевод в другой формат для вектаризации.\n",
    "Вообще скорее всего переводт было не обязательно, достаточно найт нормальный метод из предложенных разделения на чанки text_splitter  \n",
    "https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/\n",
    "\n",
    "тут возникает проблема\n",
    "1) если не переводить в формат Document то есть проблемы с чанками которую можно решить поменяв разделитель, читайте инф по ссылке\n",
    "2) есили всёже переводить что я делаю, появляюся лишние символы, множество точек, из - за чего чанки становятся бессмысленными просмотрите\n",
    "    * cleaned_content - плохо но прдобработаны\n",
    "    * documents - получившийся из них с точками /н и подобной ересью \n",
    "\n",
    "нужно разобраться, как разбить на чанки не потеряв обработку данных на чистоту"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e41bb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = []\n",
    "\n",
    "# Проходим по каждой строке в cleaned_content\n",
    "i = 0\n",
    "for content in cleaned_content:\n",
    "    # Создаем новый документ для каждой строки\n",
    "    i += 1\n",
    "    documents.append(Document(metadata={'source': 'source_info', 'page': i}, page_content=content))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fc5ea1",
   "metadata": {},
   "source": [
    "### Разбиение на чанки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f58bba",
   "metadata": {},
   "source": [
    "нужно поэксперементировать с данными и просмотреть чанки, на данный момент они несут практически 0 смысл\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "09d07b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=250,  # Максимальный размер части\n",
    "    chunk_overlap=0  # Перекрытие между частями\n",
    ")\n",
    "\n",
    "# Разделяем содержимое каждого документа\n",
    "split_documents = []\n",
    "for doc in documents:\n",
    "    split_parts = text_splitter.split_text(doc.page_content)\n",
    "    for part in split_parts:\n",
    "        split_documents.append(Document(metadata=doc.metadata, page_content=part))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb5a1dd",
   "metadata": {},
   "source": [
    "### Векторизация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0da89c",
   "metadata": {},
   "source": [
    "требует больших мощностей, нужно запускать с мощьного пк, посмотреть возможность работы на видюхе, поспрашивайте гпт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "adc0ca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаём путь до места хранения\n",
    "from pathlib import Path\n",
    "\n",
    "# Получаем текущий рабочий каталог\n",
    "current_dir = Path.cwd()\n",
    "\n",
    "# Создаем путь к папке processed_data, которая находится на уровне выше текущего каталога\n",
    "processed_data_path = current_dir.parent / 'data' / 'processed_data' / 'union.parquet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ea769b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# Create embeddings for documents and store them in a vector store\n",
    "vectorstore = SKLearnVectorStore.from_documents(\n",
    "    documents=split_documents[:5],\n",
    "    embedding = OllamaEmbeddings(model=\"llama3.1\"),\n",
    "    persist_path= processed_data_path,  # for save\n",
    "    serializer=\"parquet\" # for save\n",
    ")\n",
    "retriever = vectorstore.as_retriever(k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9663ca",
   "metadata": {},
   "source": [
    "### Сохранение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d7bec6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "vectorstore.persist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
